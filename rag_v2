  # senior_rag.py
  import argparse
  import logging
  import sys
  from typing import List

  import bs4
  from dotenv import load_dotenv
  from langchain.chat_models import init_chat_model
  from langchain_community.document_loaders import WebBaseLoader
  from langchain_core.documents import Document
  from langchain_core.prompts import ChatPromptTemplate
  from langchain_core.vectorstores import InMemoryVectorStore
  from langchain_text_splitters import RecursiveCharacterTextSplitter
  from langchain_openai import OpenAIEmbeddings
  from langchainhub import Client
  from langgraph.graph import START, StateGraph

  logging.basicConfig(
      level=logging.INFO,
      format="%(asctime)s %(levelname)s %(name)s: %(message)s",
  )
  logger = logging.getLogger(__name__)
  logging.getLogger("langchain").setLevel(logging.WARNING)

  load_dotenv()


  class LLM:
      def __init__(self, provider: str, model: str):
          self.provider = provider
          self.model = model
          self.chain = ""

      def assign_chain(self, chain):
          self.chain = chain

      def chat_model(self):
          try:
              return init_chat_model(self.model, model_provider=self.provider)
          except Exception as exc:
              raise RuntimeError(
                  f"Failed to initialize chat model '{self.model}' for provider '{self.provider}'."
              ) from exc


  class State(dict):
      question: str
      context: List[Document]
      answer: str


  class RAG:
      def __init__(self, llm, embeddings_model: str = "text-embedding-ada-002"):
          self.embeddings_model = embeddings_model
          try:
              self.embeddings = OpenAIEmbeddings(model=self.embeddings_model)
          except Exception as exc:
              raise RuntimeError(
                  f"Failed to initialize embeddings model '{self.embeddings_model}'."
              ) from exc

          self.vector_store = InMemoryVectorStore(self.embeddings)
          self.docs: List[Document] = []
          self.llm = llm
          self._has_ingested = False

          self.prompt = self._safe_prompt_pull(
              system_msg="Use the context to answer the user. If unsure, say you are unsure.",
              human_msg="Question: {question}\n\nContext:\n{context}",
          )

      @staticmethod
      def _safe_prompt_pull(system_msg: str, human_msg: str):
          try:
              hub_client = Client()
              prompt = hub_client.pull("rlm/rag-prompt")
              logger.info("Loaded prompt template from LangChain Hub.")
              return prompt
          except Exception as exc:
              logger.info("Falling back to local prompt template (%s).", exc)
              return ChatPromptTemplate.from_messages(
                  [
                      ("system", system_msg),
                      ("human", human_msg),
                  ]
              )

      def load_url_data(self, url: str):
          loader = WebBaseLoader(
              web_paths=(url,),
              bs_kwargs=dict(
                  parse_only=bs4.SoupStrainer(
                      class_=("post-content", "post-title", "post-header")
                  )
              ),
          )
          try:
              docs = loader.load()
          except Exception as exc:
              raise RuntimeError(f"Failed to load content from {url}") from exc

          if not docs:
              logger.warning("No documents retrieved from %s.", url)
              return

          self.docs.extend(docs)
          logger.info("Loaded %d document(s) from %s.", len(docs), url)

      def generate_split_docs(self, chunk_size: int = 1000, chunk_overlap: int = 200):
          if not self.docs:
              logger.warning("No source documents available to split.")
              return []

          text_splitter = RecursiveCharacterTextSplitter(
              chunk_size=chunk_size, chunk_overlap=chunk_overlap
          )
          splits = text_splitter.split_documents(self.docs)
          logger.info("Generated %d chunk(s) for embedding.", len(splits))
          return splits

      def generate_embeddings(self):
          if self._has_ingested:
              logger.debug("Embeddings already generated; skipping re-ingest.")
              return

          splits = self.generate_split_docs()
          if not splits:
              raise RuntimeError("No document chunks produced; cannot build vector store.")

          self.vector_store.add_documents(documents=splits)
          self._has_ingested = True
          logger.info("Ingested %d chunk(s) into the vector store.", len(splits))

      def retrieve(self, state: State):
          if not self._has_ingested:
              raise RuntimeError("Vector store is empty; call generate_embeddings first.")
          retrieved_docs = self.vector_store.similarity_search(state["question"])
          return {"context": retrieved_docs}

      def generate(self, state: State):
          docs_content = "\n\n".join(doc.page_content for doc in state["context"])
          messages = self.prompt.invoke(
              {"question": state["question"], "context": docs_content}
          )
          response = self.llm.invoke(messages)
          return {"answer": response.content}

      def prepare_graph(self):
          graph_builder = StateGraph(State).add_sequence([self.retrieve, self.generate])
          graph_builder.add_edge(START, "retrieve")
          return graph_builder.compile()


  def run(question: str, url: str, provider: str, model: str):
      llm_wrapper = LLM(provider, model)
      llm = llm_wrapper.chat_model()

      rag_pipeline = RAG(llm)

      logger.info("Loading URL data...")
      rag_pipeline.load_url_data(url)

      logger.info("Generating embeddings...")
      rag_pipeline.generate_embeddings()

      logger.info("Preparing graph...")
      graph = rag_pipeline.prepare_graph()

      logger.info("Invoking graph...")
      response = graph.invoke({"question": question})
      return response["answer"]


  def build_parser():
      parser = argparse.ArgumentParser(
          description="Run a minimal RAG pipeline over a single URL."
      )
      parser.add_argument("question", help="Question to ask against the retrieved context.")
      parser.add_argument(
          "--url",
          default="https://lilianweng.github.io/posts/2023-06-23-agent/",
          help="Source URL to ingest.",
      )
      parser.add_argument("--provider", default="google_genai", help="LLM provider name.")
      parser.add_argument("--model", default="gemini-2.0-flash", help="LLM model name.")
      return parser


  def main(argv: List[str]):
      parser = build_parser()
      args = parser.parse_args(argv)

      try:
          answer = run(args.question, args.url, args.provider, args.model)
      except Exception as exc:
          logger.error("Pipeline run failed: %s", exc)
          sys.exit(1)

      print(answer)


  if __name__ == "__main__":
      main(sys.argv[1:])

